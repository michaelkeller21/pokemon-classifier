{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am most interested in the work of the connectionists and neural networks. In this report, I will attempt to answer the following question:\n",
    "\n",
    "Given an image of a Pokemon, can a convolutional neural network model be used to classify the Pokemon by type?\n",
    "\n",
    "I had this inspiration from a dataset I recently found on Kaggle, shown here:\n",
    "\n",
    "https://www.kaggle.com/vishalsubbiah/pokemon-images-and-types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I refer the reader to the following Wikipedia article for more information on Pokemon:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Pok%C3%A9mon_(video_game_series)\n",
    "\n",
    "For the purposes of context for this report, Pokemon is shorthand for Pocket Monsters, and it is a Japanese video game series developed for Nintendo gaming systems. In it, the player goes on an adventure where they assemble a team of 6 creatures, train them up to become strong, and compete for the recognition of becoming the most powerful trainer in the game world. Players in Pokemon compete by battling them against each other. As of this writing, there are 890 unique Pokemon. The Kaggle dataset mentioned above contains only 809 Pokemon, and was not updated for the additional 81 Pokemon introduced in Pokemon Sword and Shield in November 2019.\n",
    "\n",
    "Each Pokemon has a primary and possibly a secondary type. Not all Pokemon have a seconday type. For the purposes of simplicity, we will only use the primary type of a Pokemon as a class label. There are 18 unique types of Pokemon, which each type having its own strengths and weaknesses in battle with respect to other types. Examples of types include Fire, Water, Grass, Ground, or Electric. We will investigate in this report the ability of a CNN to distinguish Pokemon by type based on their appearance in images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1- Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/mkell/Dropbox/Spring 2020/Artificial Intelligence/pokemon-type-classifier/pokemon-classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['Bug', 'Dark', 'Dragon', 'Electric', 'Fairy', 'Fighting', 'Fire',\n",
      "       'Flying', 'Ghost', 'Grass', 'Ground', 'Ice', 'Normal', 'Poison',\n",
      "       'Psychic', 'Rock', 'Steel', 'Water'], dtype=object), array([ 72,  29,  27,  40,  18,  29,  53,   3,  27,  78,  32,  23, 105,\n",
      "        34,  53,  46,  26, 114], dtype=int64))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type1</th>\n",
       "      <th>Type2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abomasnow</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Ice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abra</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>absol</td>\n",
       "      <td>Dark</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accelgor</td>\n",
       "      <td>Bug</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aegislash-blade</td>\n",
       "      <td>Steel</td>\n",
       "      <td>Ghost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>zoroark</td>\n",
       "      <td>Dark</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>zorua</td>\n",
       "      <td>Dark</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>zubat</td>\n",
       "      <td>Poison</td>\n",
       "      <td>Flying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>zweilous</td>\n",
       "      <td>Dark</td>\n",
       "      <td>Dragon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>zygarde-50</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>Ground</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>809 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name    Type1   Type2\n",
       "0          abomasnow    Grass     Ice\n",
       "1               abra  Psychic     NaN\n",
       "2              absol     Dark     NaN\n",
       "3           accelgor      Bug     NaN\n",
       "4    aegislash-blade    Steel   Ghost\n",
       "..               ...      ...     ...\n",
       "804          zoroark     Dark     NaN\n",
       "805            zorua     Dark     NaN\n",
       "806            zubat   Poison  Flying\n",
       "807         zweilous     Dark  Dragon\n",
       "808       zygarde-50   Dragon  Ground\n",
       "\n",
       "[809 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pokemon=pd.read_csv('pokemon.csv')\n",
    "print(np.unique(pokemon['Type1'], return_counts=True))\n",
    "pokemon=pokemon.sort_values('Name')\n",
    "pokemon=pokemon.reset_index(drop=True)\n",
    "pokemon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original dataset of 809 Pokemon. We will use the column 'Type1' for labeling. Next we load in the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=np.empty((len(os.listdir('images/original_images')), 120, 120, 3))\n",
    "count=0\n",
    "\n",
    "for root, dirs, files in os.walk('images/original_images'):\n",
    "    for i, file in enumerate(files):        \n",
    "        path = os.path.join(root, file) \n",
    "        img=cv2.imread(path)        \n",
    "        images[count] = img        \n",
    "        count=count+1        \n",
    "        #print(\"Loaded file \"+str(count)+ \" of \"+str(len(os.listdir('images/images')))+ \" \")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(809, 120, 120, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Pokemon types are intuitive and some are not. For example, the first and third Pokemon in the original dataset are Ice-type Pokemon, which is supported by their white, snowy appearance. However, the second Pokemon is Psychic-type, which is not immediately evident from its appearance. The difficulty of this task for humans is present because of this fact. Thus, we hope to see how difficult this task is for a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the original dataset contains only 809 images in 18 classes. This is hardly enough data with which to train a model. Nevertheless, we will try and train a model on this small dataset to see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we preprocess the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create labels\n",
    "image_labels=np.array(pokemon['Type1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "images/=255\n",
    "images=images.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "image_labels = label_encoder.fit_transform(image_labels)\n",
    "# one hot encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "image_labels = image_labels.reshape(len(image_labels), 1)\n",
    "image_labels = onehot_encoder.fit_transform(image_labels)\n",
    "image_labels = np.asarray(image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train/test sets\n",
    "train_data, test_data, train_labels, test_labels=train_test_split(images, image_labels, test_size=0.3, shuffle=True)\n",
    "train_data, val_data, train_labels, val_labels=train_test_split(train_data, train_labels, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the model. Our input images are of size (120, 120, 3), as they are 120x120 RGB images. We use a Conv-Pool-Conv-Pool format for the network, doubling the number of filters in each convolutional layer. Once we have reduced the output to 512 1x1 images, we flatten the convolutional output, we use 3 Dense layers at the end of the network with 256, 128, and 64 nodes before pssing the output to our final softmax layer of 18 classes. All layers of the neural network except the final output layer have a Rectified Linear Unit, or ReLU activation function.\n",
    "\n",
    "These choices for model architecture were based on past convolutional neural network designs in the field. For an optimizer, we use Adam, or adaptive gradient descent with momentum. This is the most widely accepted optimizer for convolutional neural networks in the literature. We use a learning rate of 0.0001 for the network. This was determined through trial and error of training the network. We train the network with 63% of the 809 Pokemon, validate it on 7% of the 809 Pokemon, and test it on 30% of the 809 Pokemon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 116, 116, 32)      2432      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 58, 58, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 56, 56, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 128)       204928    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 3, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 18)                1170      \n",
      "=================================================================\n",
      "Total params: 1,874,834\n",
      "Trainable params: 1,874,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(5, 5), activation='relu', input_shape=(120, 120, 3)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(18, activation='softmax'))\n",
    "\n",
    "adam=tf.keras.optimizers.Adam(lr=10**-4)\n",
    "\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 509 samples, validate on 57 samples\n",
      "Epoch 1/50\n",
      "509/509 [==============================] - 5s 11ms/sample - loss: 2.7904 - accuracy: 0.1159 - val_loss: 2.7313 - val_accuracy: 0.1404\n",
      "Epoch 2/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 2.7432 - accuracy: 0.1375 - val_loss: 2.7033 - val_accuracy: 0.1404\n",
      "Epoch 3/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 2.7061 - accuracy: 0.1297 - val_loss: 2.7465 - val_accuracy: 0.0526\n",
      "Epoch 4/50\n",
      "509/509 [==============================] - 2s 5ms/sample - loss: 2.6936 - accuracy: 0.1316 - val_loss: 2.7210 - val_accuracy: 0.0526\n",
      "Epoch 5/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 2.6623 - accuracy: 0.1493 - val_loss: 2.6691 - val_accuracy: 0.1404\n",
      "Epoch 6/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 2.6150 - accuracy: 0.1768 - val_loss: 2.6646 - val_accuracy: 0.1754\n",
      "Epoch 7/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 2.5806 - accuracy: 0.1709 - val_loss: 2.5917 - val_accuracy: 0.1930\n",
      "Epoch 8/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 2.4846 - accuracy: 0.2122 - val_loss: 2.5936 - val_accuracy: 0.2105\n",
      "Epoch 9/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 2.3682 - accuracy: 0.2397 - val_loss: 2.6653 - val_accuracy: 0.1404\n",
      "Epoch 10/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 2.2358 - accuracy: 0.2908 - val_loss: 2.6678 - val_accuracy: 0.1404\n",
      "Epoch 11/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 2.0515 - accuracy: 0.3360 - val_loss: 2.7091 - val_accuracy: 0.2105\n",
      "Epoch 12/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 1.8368 - accuracy: 0.3969 - val_loss: 2.8849 - val_accuracy: 0.0877\n",
      "Epoch 13/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 1.5850 - accuracy: 0.4951 - val_loss: 3.2488 - val_accuracy: 0.1404\n",
      "Epoch 14/50\n",
      "509/509 [==============================] - 2s 5ms/sample - loss: 1.3344 - accuracy: 0.5776 - val_loss: 3.8088 - val_accuracy: 0.0877\n",
      "Epoch 15/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 1.0043 - accuracy: 0.6857 - val_loss: 4.2210 - val_accuracy: 0.0526\n",
      "Epoch 16/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.8055 - accuracy: 0.7485 - val_loss: 4.1426 - val_accuracy: 0.0877\n",
      "Epoch 17/50\n",
      "509/509 [==============================] - 2s 5ms/sample - loss: 0.5908 - accuracy: 0.8173 - val_loss: 4.8775 - val_accuracy: 0.1053\n",
      "Epoch 18/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.4581 - accuracy: 0.8546 - val_loss: 4.9178 - val_accuracy: 0.1053\n",
      "Epoch 19/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.3612 - accuracy: 0.9018 - val_loss: 5.0595 - val_accuracy: 0.1053\n",
      "Epoch 20/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.4014 - accuracy: 0.8625 - val_loss: 7.2022 - val_accuracy: 0.1053\n",
      "Epoch 21/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.2903 - accuracy: 0.9214 - val_loss: 5.6238 - val_accuracy: 0.1053\n",
      "Epoch 22/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.2541 - accuracy: 0.9371 - val_loss: 6.0118 - val_accuracy: 0.0877\n",
      "Epoch 23/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.3070 - accuracy: 0.9234 - val_loss: 6.1755 - val_accuracy: 0.1053\n",
      "Epoch 24/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.1261 - accuracy: 0.9627 - val_loss: 7.6448 - val_accuracy: 0.0702\n",
      "Epoch 25/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.1147 - accuracy: 0.9686 - val_loss: 8.5017 - val_accuracy: 0.0526\n",
      "Epoch 26/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.3075 - accuracy: 0.9214 - val_loss: 5.9130 - val_accuracy: 0.1053\n",
      "Epoch 27/50\n",
      "509/509 [==============================] - 2s 5ms/sample - loss: 0.2575 - accuracy: 0.9293 - val_loss: 6.4049 - val_accuracy: 0.0702\n",
      "Epoch 28/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.1029 - accuracy: 0.9705 - val_loss: 7.6685 - val_accuracy: 0.1053\n",
      "Epoch 29/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.1791 - accuracy: 0.9411 - val_loss: 8.2560 - val_accuracy: 0.1053\n",
      "Epoch 30/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.2425 - accuracy: 0.9352 - val_loss: 6.9606 - val_accuracy: 0.0702\n",
      "Epoch 31/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.0815 - accuracy: 0.9804 - val_loss: 7.9533 - val_accuracy: 0.1053\n",
      "Epoch 32/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.1421 - accuracy: 0.9646 - val_loss: 6.7992 - val_accuracy: 0.0702\n",
      "Epoch 33/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.1469 - accuracy: 0.9587 - val_loss: 7.2109 - val_accuracy: 0.0877\n",
      "Epoch 34/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.0561 - accuracy: 0.9804 - val_loss: 7.9610 - val_accuracy: 0.1404\n",
      "Epoch 35/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.1181 - accuracy: 0.9705 - val_loss: 7.4330 - val_accuracy: 0.0526\n",
      "Epoch 36/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.0165 - accuracy: 0.9980 - val_loss: 8.9522 - val_accuracy: 0.0526\n",
      "Epoch 37/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.0059 - accuracy: 1.0000 - val_loss: 9.4915 - val_accuracy: 0.0526\n",
      "Epoch 38/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.0207 - accuracy: 0.9941 - val_loss: 10.3578 - val_accuracy: 0.0702\n",
      "Epoch 39/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 10.6215 - val_accuracy: 0.0702\n",
      "Epoch 40/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 6.7247e-04 - accuracy: 1.0000 - val_loss: 10.8836 - val_accuracy: 0.0351\n",
      "Epoch 41/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 3.4454e-04 - accuracy: 1.0000 - val_loss: 11.2257 - val_accuracy: 0.0526\n",
      "Epoch 42/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 2.2661e-04 - accuracy: 1.0000 - val_loss: 11.4404 - val_accuracy: 0.0351\n",
      "Epoch 43/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 1.5895e-04 - accuracy: 1.0000 - val_loss: 11.6694 - val_accuracy: 0.0526\n",
      "Epoch 44/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 1.1021e-04 - accuracy: 1.0000 - val_loss: 11.9421 - val_accuracy: 0.0351\n",
      "Epoch 45/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 8.5771e-05 - accuracy: 1.0000 - val_loss: 12.1437 - val_accuracy: 0.0351\n",
      "Epoch 46/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 5.9806e-05 - accuracy: 1.0000 - val_loss: 12.3870 - val_accuracy: 0.0351\n",
      "Epoch 47/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 4.4914e-05 - accuracy: 1.0000 - val_loss: 12.5721 - val_accuracy: 0.0526\n",
      "Epoch 48/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 3.4917e-05 - accuracy: 1.0000 - val_loss: 12.8199 - val_accuracy: 0.0351\n",
      "Epoch 49/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 2.6081e-05 - accuracy: 1.0000 - val_loss: 13.0631 - val_accuracy: 0.0351\n",
      "Epoch 50/50\n",
      "509/509 [==============================] - 3s 5ms/sample - loss: 2.0865e-05 - accuracy: 1.0000 - val_loss: 13.4131 - val_accuracy: 0.0351\n"
     ]
    }
   ],
   "source": [
    "mc=tf.keras.callbacks.ModelCheckpoint('best_pokemon_model_original.hdf5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "hist=model.fit(train_data, train_labels, batch_size=1, epochs=30, verbose=1, callbacks=[mc], \n",
    "               validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.724155082624145, 0.12757201]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=tf.keras.models.load_model('best_pokemon_model_original.hdf5')\n",
    "test_results=model.evaluate(test_data, test_labels, verbose=0)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have 18 classes of Pokemon type, if a network were randomly guessing, it would achieve an accuracy of 1/18=0.055. Achieving a test accuracy of 23% thus means the network is doing better than randomly guessing, though still has fairly low accuracy. This supports the conclusion that there are noticeable, yet inconsistent patterns in Pokemon appearance that signify type.\n",
    "\n",
    "A dataset of 809 instances is likely too small to properly train a model. We also notice that the 18 classes in this dataset are imbalanced. The most common class is 'Water' with 114 instances, and the least common class is 'Flying' with 3 instances. We can attempt to resolve this through oversampling, making copies of existing samples. We do this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2- Oversampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must balance the dataset. We first automatically label each image based on its classification from the Pokemon csv file, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('images/labeled_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels=np.unique(pokemon['Type1'])\n",
    "\n",
    "#for label in labels:\n",
    "    #os.mkdir('images/labeled_images/'+label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('images/labeled_images')\n",
    "\n",
    "#labels=np.unique(pokemon['Type1'])\n",
    "\n",
    "#for label in labels:\n",
    "    #for i in range(len(pokemon)):\n",
    "        #if pokemon['Type1'][i]==label:\n",
    "            #cv2.imwrite(label+'/'+pokemon['Name'][i]+'.png', images[i])            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we balance the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_images=np.empty((len(os.listdir('images/labeled_images/')), 120, 120, 3))\n",
    "count=0\n",
    "\n",
    "filenames=[]\n",
    "\n",
    "for root, dirs, files in os.walk('images/labeled_images/Bug'):\n",
    "    for i, file in enumerate(files):        \n",
    "        path = os.path.join(root, file) \n",
    "        img=cv2.imread(path)        \n",
    "        labeled_images[count] = img        \n",
    "        count=count+1\n",
    "        filenames.append(file)\n",
    "\n",
    "i=2\n",
    "while len(os.listdir('images/labeled_images/Bug'))<1000:    \n",
    "    for j in range(len(labeled_images)):\n",
    "        cv2.imwrite('images/labeled_images/Bug/'+filenames[j]+'_'+str(i)+'.png', labeled_images[j])\n",
    "        if len(os.listdir('images/labeled_images/Bug'))<1000:\n",
    "            break\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/vishalsubbiah/pokemon-images-and-types\n",
    "    \n",
    "https://en.wikipedia.org/wiki/Pok%C3%A9mon_(video_game_series)\n",
    "\n",
    "https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-ii-hyper-parameter-42efca01e5d7\n",
    "\n",
    "https://www.youtube.com/watch?v=g2vlqhefADk&t=273s\n",
    "\n",
    "https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/\n",
    "\n",
    "https://towardsdatascience.com/deep-learning-unbalanced-training-data-solve-it-like-this-6c528e9efea6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
